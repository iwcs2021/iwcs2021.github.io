<!doctype html>
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<!--  EDIT FROM HERE -->
<title>Proceedings of the 1st Workshop on Multimodal Semantic Representations (MMSR)</title>
<!--  EDIT TO HERE -->
<link rel="stylesheet" rev="stylesheet" href="../standard.css">
</head>
<body bgcolor="white">
<center>
<table cellspacing="0" cellpadding="0" border="0">
  <tr>
    <td colspan="3"><a href="index.html">MMSR 2021 Proceedings Home</a> | <a href="https://mmsr-workshop.github.io/">MMSR 2021 WEBSITE</a> | <a href="http://www.aclweb.org/">ACL WEBSITE</a></td>
  </tr>
</table>
<br>

<h2>Proceedings of the 1st Workshop on Multimodal Semantic Representations (MMSR)</h2>
<p><b>Chairs</b><br>
Lucia Donatelli, Nikhil Krishnaswamy, Kenneth Lai, and James Pustejovsky
</p>
<table cellspacing="0" cellpadding="5" border="1">
  <tr class="bg1">
    <td><table cellspacing="0" cellpadding="5" border="0">
        <tr align=center>
          <td><b><a href="../book.pdf">Full proceedings volume</a> (PDF)</b></td>
        </tr>
        <tr align=center>
          <td><a href="program.html">Schedule</a> and <a href="authors.html">author index</a> (HTML)</td>
        </tr>
        <tr align=center>
          <td><a href="2021.mmsr-1.0.bib">Bibliography</a> (BibTeX)</td>
        </tr>
        <tr align=center>
          <td><a href="https://mmsr-workshop.github.io/">Live website</a></td>
        </tr>
      </table>
    </td>
  </tr>
</table>

<br></br><table cellspacing="0" cellpadding="5" border="1">
  <tr class="bg1">
    <td valign="top"><a href="pdf/2021.mmsr-1.0.pdf">pdf</a></td>
    <td valign="top"><a href="bib/2021.mmsr-1.0.bib">bib</a></td>
    <td valign="top"><a href="pdf/2021.mmsr-1.0.pdf"><b>Front matter<b></a></td>
    <td valign="top">pages</td>
  </tr>
		<tr class="bg2">
			<td valign="top"><a href="pdf/2021.mmsr-1.1.pdf">pdf</a></td>
			<td valign="top"><a href="bib/2021.mmsr-1.1.bib">bib</a></td>
			<td valign="top" align="left"><a href="pdf/2021.mmsr-1.1.pdf"><i>What is Multimodality?</i></a><br> Letitia  Parcalabescu,  Nils  Trost and  Anette  Frank </td>
			<td valign="top" align="left"><a name="1">pp.&nbsp;1&#8209;10</a></td>
		</tr>

		<tr class="bg1">
			<td valign="top"><a href="pdf/2021.mmsr-1.2.pdf">pdf</a></td>
			<td valign="top"><a href="bib/2021.mmsr-1.2.bib">bib</a></td>
			<td valign="top" align="left"><a href="pdf/2021.mmsr-1.2.pdf"><i>Are Gestures Worth a Thousand Words? An Analysis of Interviews in the Political Domain</i></a><br> Daniela  Trotta and  Sara  Tonelli </td>
			<td valign="top" align="left"><a name="11">pp.&nbsp;11&#8209;20</a></td>
		</tr>

		<tr class="bg2">
			<td valign="top"><a href="pdf/2021.mmsr-1.3.pdf">pdf</a></td>
			<td valign="top"><a href="bib/2021.mmsr-1.3.bib">bib</a></td>
			<td valign="top" align="left"><a href="pdf/2021.mmsr-1.3.pdf"><i>Requesting clarifications with speech and gestures</i></a><br> Jonathan  Ginzburg and  Andy  Luecking </td>
			<td valign="top" align="left"><a name="21">pp.&nbsp;21&#8209;31</a></td>
		</tr>

		<tr class="bg1">
			<td valign="top"><a href="pdf/2021.mmsr-1.4.pdf">pdf</a></td>
			<td valign="top"><a href="bib/2021.mmsr-1.4.bib">bib</a></td>
			<td valign="top" align="left"><a href="pdf/2021.mmsr-1.4.pdf"><i>Seeing past words: Testing the cross-modal capabilities of pretrained V&amp;L models on counting tasks</i></a><br> Letitia  Parcalabescu,  Albert  Gatt,  Anette  Frank and  Iacer  Calixto </td>
			<td valign="top" align="left"><a name="32">pp.&nbsp;32&#8209;44</a></td>
		</tr>

		<tr class="bg2">
			<td valign="top"><a href="pdf/2021.mmsr-1.5.pdf">pdf</a></td>
			<td valign="top"><a href="bib/2021.mmsr-1.5.bib">bib</a></td>
			<td valign="top" align="left"><a href="pdf/2021.mmsr-1.5.pdf"><i>How Vision Affects Language: Comparing Masked Self-Attention in Uni-Modal and Multi-Modal Transformer</i></a><br> Nikolai  Ilinykh and  Simon  Dobnik </td>
			<td valign="top" align="left"><a name="45">pp.&nbsp;45&#8209;55</a></td>
		</tr>

		<tr class="bg1">
			<td valign="top"><a href="pdf/2021.mmsr-1.6.pdf">pdf</a></td>
			<td valign="top"><a href="bib/2021.mmsr-1.6.bib">bib</a></td>
			<td valign="top" align="left"><a href="pdf/2021.mmsr-1.6.pdf"><i>EMISSOR: A platform for capturing multimodal interactions as Episodic Memories and Interpretations with Situated Scenario-based Ontological References</i></a><br> Selene  Baez Santamaria,  Thomas  Baier,  Taewoon  Kim,  Lea  Krause,  Jaap  Kruijt and  Piek  Vossen </td>
			<td valign="top" align="left"><a name="56">pp.&nbsp;56&#8209;77</a></td>
		</tr>

		<tr class="bg2">
			<td valign="top"><a href="pdf/2021.mmsr-1.7.pdf">pdf</a></td>
			<td valign="top"><a href="bib/2021.mmsr-1.7.bib">bib</a></td>
			<td valign="top" align="left"><a href="pdf/2021.mmsr-1.7.pdf"><i>Annotating anaphoric phenomena in situated dialogue</i></a><br> Sharid  Lo√°iciga,  Simon  Dobnik and  David  Schlangen </td>
			<td valign="top" align="left"><a name="78">pp.&nbsp;78&#8209;88</a></td>
		</tr>

		<tr class="bg1">
			<td valign="top"><a href="pdf/2021.mmsr-1.8.pdf">pdf</a></td>
			<td valign="top"><a href="bib/2021.mmsr-1.8.bib">bib</a></td>
			<td valign="top" align="left"><a href="pdf/2021.mmsr-1.8.pdf"><i>Incremental Unit Networks for Multimodal, Fine-grained Information State Representation</i></a><br> Casey  Kennington and  David  Schlangen </td>
			<td valign="top" align="left"><a name="89">pp.&nbsp;89&#8209;94</a></td>
		</tr>

		<tr class="bg2">
			<td valign="top"><a href="pdf/2021.mmsr-1.9.pdf">pdf</a></td>
			<td valign="top"><a href="bib/2021.mmsr-1.9.bib">bib</a></td>
			<td valign="top" align="left"><a href="pdf/2021.mmsr-1.9.pdf"><i>Teaching Arm and Head Gestures to a Humanoid Robot through Interactive Demonstration and Spoken Instruction</i></a><br> Michael  Brady and  Han  Du </td>
			<td valign="top" align="left"><a name="95">pp.&nbsp;95&#8209;101</a></td>
		</tr>

		<tr class="bg1">
			<td valign="top"><a href="pdf/2021.mmsr-1.10.pdf">pdf</a></td>
			<td valign="top"><a href="bib/2021.mmsr-1.10.bib">bib</a></td>
			<td valign="top" align="left"><a href="pdf/2021.mmsr-1.10.pdf"><i>Building a Video-and-Language Dataset with Human Actions for Multimodal Logical Inference</i></a><br> Riko  Suzuki,  Hitomi  Yanaka,  Koji  Mineshima and  Daisuke  Bekki </td>
			<td valign="top" align="left"><a name="102">pp.&nbsp;102&#8209;107</a></td>
		</tr>

</table><p>
Last modified on June 3, 2021, 9:30 a.m.<p>&nbsp;
</center>
</body>
</html>
