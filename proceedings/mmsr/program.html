<!doctype html>
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>Program - The 1st Workshop on Multimodal Semantic Representations (MMSR)</title>
<link rel="stylesheet" rev="stylesheet" href="../standard.css">
</head>
<body bgcolor="white">

<center>
<table cellspacing="0" cellpadding="0" border="0">
	<tr>
                <td colspan="3"><a href="index.html">MMSR 2021 Proceedings Home</a> | <a href="https://mmsr-workshop.github.io/">MMSR 2021 WEBSITE</a> | <a href="http://www.aclweb.org/">ACL WEBSITE</a></td>
	</tr>
</table>

<br><br>

<h2>The 1st Workshop on Multimodal Semantic Representations (MMSR)</h2>

<p><b>WORKSHOP PROGRAM</b></p>

<table cellspacing="0" cellpadding="5" border="0"><tr><td colspan=2 style="padding-top: 14px;"><h4>Wednesday, June 16, 2021</h4></td></tr>
<tr><td valign=top style="padding-top: 14px;"><b>16:00–16:15</b></td><td valign=top style="padding-top: 14px;"><b><em>Introduction</em></b></td></tr>
<tr><td valign=top>16:15–17:00</td><td valign=top><em>Invited Talk: From action to language through gesture</em><br>Virginia Volterra and Chiara Bonsignori</td></tr>
<tr><td valign=top style="padding-top: 14px;">&nbsp;</td><td valign=top style="padding-top: 14px;"><b>17:05–17:35 Oral Session 1</b></td></tr>
<tr><td valign=top width=100>&nbsp;</td><td valign=top align=left><a href="pdf/2021.mmsr-1.1.pdf"><i>What is Multimodality?</i></a><br>
Letitia Parcalabescu, Nils Trost and Anette Frank</td></tr>
<tr><td valign=top width=100>&nbsp;</td><td valign=top align=left><a href="pdf/2021.mmsr-1.2.pdf"><i>Are Gestures Worth a Thousand Words? An Analysis of Interviews in the Political Domain</i></a><br>
Daniela Trotta and Sara Tonelli</td></tr>
<tr><td valign=top width=100>&nbsp;</td><td valign=top align=left><a href="pdf/2021.mmsr-1.3.pdf"><i>Requesting clarifications with speech and gestures</i></a><br>
Jonathan Ginzburg and Andy Luecking</td></tr>
<tr><td valign=top>17:40–18:25</td><td valign=top><em>Invited Talk: Attention, Incrementality, and Meaning: On the Interplay between Language and Vision in Reference Resolution</em><br>Matthias Scheutz</td></tr>
<tr><td valign=top style="padding-top: 14px;">&nbsp;</td><td valign=top style="padding-top: 14px;"><b>18:30–19:10 Oral Session 2</b></td></tr>
<tr><td valign=top width=100>&nbsp;</td><td valign=top align=left><a href="pdf/2021.mmsr-1.4.pdf"><i>Seeing past words: Testing the cross-modal capabilities of pretrained V&amp;L models on counting tasks</i></a><br>
Letitia Parcalabescu, Albert Gatt, Anette Frank and Iacer Calixto</td></tr>
<tr><td valign=top width=100>&nbsp;</td><td valign=top align=left><a href="pdf/2021.mmsr-1.5.pdf"><i>How Vision Affects Language: Comparing Masked Self-Attention in Uni-Modal and Multi-Modal Transformer</i></a><br>
Nikolai Ilinykh and Simon Dobnik</td></tr>
<tr><td valign=top width=100>&nbsp;</td><td valign=top align=left><a href="pdf/2021.mmsr-1.6.pdf"><i>EMISSOR: A platform for capturing multimodal interactions as Episodic Memories and Interpretations with Situated Scenario-based Ontological References</i></a><br>
Selene Baez Santamaria, Thomas Baier, Taewoon Kim, Lea Krause, Jaap Kruijt and Piek Vossen</td></tr>
<tr><td valign=top width=100>&nbsp;</td><td valign=top align=left><a href="pdf/2021.mmsr-1.7.pdf"><i>Annotating anaphoric phenomena in situated dialogue</i></a><br>
Sharid Loáiciga, Simon Dobnik and David Schlangen</td></tr>
<tr><td valign=top style="padding-top: 14px;">&nbsp;</td><td valign=top style="padding-top: 14px;"><b>19:15–19:45 Poster Session</b></td></tr>
<tr><td valign=top width=100>&nbsp;</td><td valign=top align=left><a href="pdf/2021.mmsr-1.8.pdf"><i>Incremental Unit Networks for Multimodal, Fine-grained Information State Representation</i></a><br>
Casey Kennington and David Schlangen</td></tr>
<tr><td valign=top width=100>&nbsp;</td><td valign=top align=left><a href="pdf/2021.mmsr-1.9.pdf"><i>Teaching Arm and Head Gestures to a Humanoid Robot through Interactive Demonstration and Spoken Instruction</i></a><br>
Michael Brady and Han Du</td></tr>
<tr><td valign=top width=100>&nbsp;</td><td valign=top align=left><a href="pdf/2021.mmsr-1.10.pdf"><i>Building a Video-and-Language Dataset with Human Actions for Multimodal Logical Inference</i></a><br>
Riko Suzuki, Hitomi Yanaka, Koji Mineshima and Daisuke Bekki</td></tr>
<tr><td valign=top style="padding-top: 14px;"><b>19:45–20:00</b></td><td valign=top style="padding-top: 14px;"><b><em>Closing</em></b></td></tr>
</table></center><p>&nbsp;</body></html>
