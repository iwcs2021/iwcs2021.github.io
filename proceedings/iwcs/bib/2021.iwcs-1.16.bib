@InProceedings{noble-maraev:2021:IWCS,
  author    = {Noble, Bill  and  Maraev, Vladislav},
  title     = {Large-scale text pre-training helps with dialogue act recognition, but not without fine-tuning},
  booktitle      = {Proceedings of the 14th International Conference on Computational Semantics (IWCS)},
  month          = {June},
  year           = {2021},
  address        = {Groningen, The Netherlands (online)},
  publisher      = {Association for Computational Linguistics},
  pages     = {166--172},
  abstract  = {We use dialogue act recognition (DAR) to investigate how well BERT represents utterances in dialogue, and how fine-tuning and large-scale pre-training contribute to its performance. We find that while both the standard BERT pre-training and pretraining on dialogue-like data are useful, task-specific fine-tuning is essential for good performance.},
  url       = {https://www.aclweb.org/anthology/2021.iwcs-1.16}
}

