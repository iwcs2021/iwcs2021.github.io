@InProceedings{betz-voigt-richardson:2021:IWCS,
  author    = {Betz, Gregor  and  Voigt, Christian  and  Richardson, Kyle},
  title     = {Critical Thinking for Language Models},
  booktitle      = {Proceedings of the 14th International Conference on Computational Semantics (IWCS)},
  month          = {June},
  year           = {2021},
  address        = {Groningen, The Netherlands (online)},
  publisher      = {Association for Computational Linguistics},
  pages     = {63--75},
  abstract  = {This paper takes a first step towards a critical thinking curriculum for neural auto-regressive language models. We introduce a synthetic corpus of deductively valid arguments, and generate artificial argumentative texts to train CRiPT: a critical thinking intermediarily pre-trained transformer based on GPT-2. Significant transfer learning effects can be observed: Trained on three simple core schemes, CRiPT accurately completes conclusions of different, and more complex types of arguments, too. CRiPT generalizes the core argument schemes in a correct way. Moreover, we obtain consistent and promising results for NLU benchmarks. In particular, CRiPT's zero-shot accuracy on the GLUE diagnostics exceeds GPT-2's performance by 15 percentage points. The findings suggest that intermediary pre-training on texts that exemplify basic reasoning abilities (such as typically covered in critical thinking textbooks) might help language models to acquire a broad range of reasoning skills. The synthetic argumentative texts presented in this paper are a promising starting point for building such a ``critical thinking curriculum for language models.''},
  url       = {https://www.aclweb.org/anthology/2021.iwcs-1.7}
}

