@Book{NALOMA:2021,
  editor    = {Kalouli, Aikaterini-Lida  and  Moss, Lawrence S.},
  title     = {Proceedings of the 1st and 2nd Workshops on Natural Logic Meets Machine Learning (NALOMA)},
  month          = {June},
  year           = {2021},
  address        = {Groningen, the Netherlands (online)},
  publisher      = {Association for Computational Linguistics},
  url       = {https://www.aclweb.org/anthology/2021.naloma-1}
}

@InProceedings{lawley-kuehnert-schubert:2021:NALOMA,
  author    = {Lawley, Lane  and  Kuehnert, Benjamin  and  Schubert, Lenhart},
  title     = {Learning General Event Schemas with Episodic Logic},
  booktitle      = {Proceedings of the 1st and 2nd Workshops on Natural Logic Meets Machine Learning (NALOMA)},
  month          = {June},
  year           = {2021},
  address        = {Groningen, the Netherlands (online)},
  publisher      = {Association for Computational Linguistics},
  pages     = {1--6},
  abstract  = {We present a system for learning generalized, stereotypical patterns of events—or “schemas”—from natural language stories, and applying them to make predictions about other stories. Our schemas are represented with Episodic Logic, a logical form that closely mirrors natural language. By beginning with a “head start” set of protoschemas— schemas that a 1- or 2-year-old child would likely know—we can obtain useful, general world knowledge with very few story examples—often only one or two. Learned schemas can be combined into more complex, composite schemas, and used to make predictions in other stories where only partial information is available.},
  url       = {https://www.aclweb.org/anthology/2021.naloma-1.1}
}

@InProceedings{langton-srihasam:2021:NALOMA,
  author    = {Langton, John  and  Srihasam, Krishna},
  title     = {Applied Medical Code Mapping with Character-based Deep Learning Models and Word-based Logic},
  booktitle      = {Proceedings of the 1st and 2nd Workshops on Natural Logic Meets Machine Learning (NALOMA)},
  month          = {June},
  year           = {2021},
  address        = {Groningen, the Netherlands (online)},
  publisher      = {Association for Computational Linguistics},
  pages     = {7--11},
  abstract  = {Logical Observation Identifiers Names and Codes (LOINC) is a standard set of codes that enable clinicians to communicate about medical tests. Laboratories depend on LOINC to identify what tests a doctor orders for a patient. However, clinicians often use site specific, custom codes in their medical records systems that can include shorthand, spelling mistakes, and invented acronyms. Software solutions must map from these custom codes to the LOINC standard to support data interoperability. A key challenge is that LOINC is comprised of six elements. Mapping requires not only extracting those elements, but also combining them according to LOINC logic. We found that character-based deep learning excels at extracting LOINC elements while logic based methods are more effective for combining those elements into complete LOINC values. In this paper, we present an ensemble of machine learning and logic that is currently used in several medical facilities to map from},
  url       = {https://www.aclweb.org/anthology/2021.naloma-1.2}
}

@InProceedings{chen:2021:NALOMA,
  author    = {Chen, Zeming},
  title     = {Attentive Tree-structured Network for Monotonicity Reasoning},
  booktitle      = {Proceedings of the 1st and 2nd Workshops on Natural Logic Meets Machine Learning (NALOMA)},
  month          = {June},
  year           = {2021},
  address        = {Groningen, the Netherlands (online)},
  publisher      = {Association for Computational Linguistics},
  pages     = {12--21},
  abstract  = {Many state-of-art neural models designed for monotonicity reasoning perform poorly on downward inference. To address this shortcoming, we developed an attentive tree-structured neural network. It consists of a tree-based long-short-term-memory network (Tree-LSTM) with soft attention. It is designed to model the syntactic parse tree information from the sentence pair of a reasoning task. A self-attentive aggregator is used for aligning the representations of the premise and the hypothesis. We present our model and evaluate it using the Monotonicity Entailment Dataset (MED). We show and attempt to explain that our model outperforms existing models on MED.},
  url       = {https://www.aclweb.org/anthology/2021.naloma-1.3}
}

@InProceedings{traylor-pavlick-feiman:2021:NALOMA,
  author    = {Traylor, Aaron  and  Pavlick, Ellie  and  Feiman, Roman},
  title     = {Transferring Representations of Logical Connectives},
  booktitle      = {Proceedings of the 1st and 2nd Workshops on Natural Logic Meets Machine Learning (NALOMA)},
  month          = {June},
  year           = {2021},
  address        = {Groningen, the Netherlands (online)},
  publisher      = {Association for Computational Linguistics},
  pages     = {22--25},
  abstract  = {In modern natural language processing pipelines, it is common practice to "pretrain" a generative language model on a large corpus of text, and then to "finetune" the created representations by continuing to train them on a discriminative textual inference task. However, it is not immediately clear whether the logical meaning necessary to model logical entailment is captured by language models in this paradigm. We examine this pretrain-finetune recipe with language models trained on a synthetic propositional language entailment task, and present results on test sets probing models' knowledge of axioms of first order logic.},
  url       = {https://www.aclweb.org/anthology/2021.naloma-1.4}
}

@InProceedings{kim-juvekar-schubert:2021:NALOMA,
  author    = {Kim, Gene  and  Juvekar, Mandar  and  Schubert, Lenhart},
  title     = {Monotonic Inference for Underspecified Episodic Logic},
  booktitle      = {Proceedings of the 1st and 2nd Workshops on Natural Logic Meets Machine Learning (NALOMA)},
  month          = {June},
  year           = {2021},
  address        = {Groningen, the Netherlands (online)},
  publisher      = {Association for Computational Linguistics},
  pages     = {26--40},
  abstract  = {We present a method of making natural logic inferences from Unscoped Logical Form of Episodic Logic. We establish a correspondence between inference rules of scope resolved Episodic Logic and the natural logic treatment by Sánchez Valencia (1991a), and hence demonstrate the ability to handle foundational natural logic inferences from prior literature as well as more general nested monotonicity inferences.},
  url       = {https://www.aclweb.org/anthology/2021.naloma-1.5}
}

@InProceedings{rozanova-EtAl:2021:NALOMA,
  author    = {Rozanova, Julia  and  Ferreira, Deborah  and  Thayaparan, Mokanarangan  and  Valentino, Marco  and  Freitas, André},
  title     = {Supporting Context Monotonicity Abstractions in Neural NLI Models},
  booktitle      = {Proceedings of the 1st and 2nd Workshops on Natural Logic Meets Machine Learning (NALOMA)},
  month          = {June},
  year           = {2021},
  address        = {Groningen, the Netherlands (online)},
  publisher      = {Association for Computational Linguistics},
  pages     = {41--50},
  abstract  = {Natural language contexts display logical regularities with respect to substitutions of related concepts: these are captured in a functional order-theoretic property called monotonicity. For a certain class of NLI problems where the resulting entailment label depends only on the context monotonicity and the relation between the substituted concepts, we build on previous techniques that aim to improve the performance of NLI models for these problems, as consistent performance across both upward and downward monotone contexts still seems difficult to attain even for state of the art models. To this end, we reframe the problem of context monotonicity classification to make it compatible with transformer-based pre-trained NLI models and add this task to the training pipeline. Furthermore, we introduce a sound and complete simplified monotonicity logic formalism which describes our treatment of contexts as abstract units. Using the notions in our formalism, we adapt targeted challenge sets to investigate whether an intermediate context monotonicity classification task can aid NLI models’ performance on examples exhibiting monotonicity reasoning.},
  url       = {https://www.aclweb.org/anthology/2021.naloma-1.6}
}

@InProceedings{larsson-cooper:2021:NALOMA,
  author    = {Larsson, Staffan  and  Cooper, Robin},
  title     = {Bayesian Classification and Inference in a Probabilistic Type Theory with Records},
  booktitle      = {Proceedings of the 1st and 2nd Workshops on Natural Logic Meets Machine Learning (NALOMA)},
  month          = {June},
  year           = {2021},
  address        = {Groningen, the Netherlands (online)},
  publisher      = {Association for Computational Linguistics},
  pages     = {51--59},
  abstract  = {We propose a probabilistic account of semantic inference and classification formulated in terms of probabilistic type theory with records, building on Cooper et. al. (2014) and Cooper et. al. (2015). We suggest probabilistic type theoretic formulations of Naive Bayes Classifiers and Bayesian Networks. A central element of these constructions is a type-theoretic version of a random variable. We illustrate this account with a simple language game combining probabilistic classification of perceptual input with probabilistic (semantic) inference.},
  url       = {https://www.aclweb.org/anthology/2021.naloma-1.7}
}

@InProceedings{grove-bernardy-chatzikyriakidis:2021:NALOMA,
  author    = {Grove, Julian  and  Bernardy, Jean-Philippe  and  Chatzikyriakidis, Stergios},
  title     = {From compositional semantics to Bayesian pragmatics via logical inference},
  booktitle      = {Proceedings of the 1st and 2nd Workshops on Natural Logic Meets Machine Learning (NALOMA)},
  month          = {June},
  year           = {2021},
  address        = {Groningen, the Netherlands (online)},
  publisher      = {Association for Computational Linguistics},
  pages     = {60--70},
  abstract  = {Formal semantics in the Montagovian tradition provides precise meaning characterisations, but usually without a formal theory of the pragmatics of contextual parameters and their sensitivity to background knowledge. Meanwhile, formal pragmatic theories make explicit predictions about meaning in context, but generally without a well-defined compositional semantics. We propose a combined framework for the semantic and pragmatic interpretation of sentences in the face of probabilistic knowledge. We do so by (1) extending a Montagovian interpretation scheme to generate a distribution over possible meanings, and (2) generating a posterior for this distribution using a variant of the Rational Speech Act (RSA) models, but generalised to arbitrary propositions. These aspects of our framework are tied together by evaluating entailment under probabilistic uncertainty. We apply our model to anaphora resolution and show that it provides expected biases under suitable assumptions about the distributions of lexical and world-knowledge. Further, we observe that the model's output is robust to variations in its parameters within reasonable ranges.},
  url       = {https://www.aclweb.org/anthology/2021.naloma-1.8}
}

@InProceedings{kim-EtAl:2021:NALOMA,
  author    = {Kim, Gene  and  Juvekar, Mandar  and  Ekmekciu, Junis  and  Duong, Viet  and  Schubert, Lenhart},
  title     = {A (Mostly) Symbolic System for Monotonic Inference with Unscoped Episodic Logical Forms},
  booktitle      = {Proceedings of the 1st and 2nd Workshops on Natural Logic Meets Machine Learning (NALOMA)},
  month          = {June},
  year           = {2021},
  address        = {Groningen, the Netherlands (online)},
  publisher      = {Association for Computational Linguistics},
  pages     = {71--80},
  abstract  = {We implement the formalization of natural logic-like monotonic inference using Unscoped Episodic Logical Forms (ULFs) by Kim et al. (2020). We demonstrate this system's capacity to handle a variety of challenging semantic phenomena using the FraCaS dataset (Cooper et al., 1996). These results give empirical evidence for prior claims that ULF is an appropriate representation to mediate natural logic-like inferences.},
  url       = {https://www.aclweb.org/anthology/2021.naloma-1.9}
}

